<!DOCTYPE html>
<html lang="es">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title> Partes y todo | Karen Palacio</title>
    <link rel="stylesheet" href="./style.css">
    <link rel="icon" href="./favicon.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Ubuntu" />
  </head>
  <body>
  <h1 id="title"> La relación entre

	<br>
  las partes y el todo </h1>
<a id="lang" href="en.html">Read in ENGLISH</a>
<a id="lang" href="https://karen-pal.github.io/about/">Volver a página principal</a>
</br>

<img src="./process.GIF" alt="Screenshot de la pantalla mostrando un sistema con multiples videos que intervienen la imagen de laura richardson"/>
<img src="./explicacion.png" alt="Screenshot de la pantalla mostrando un sistema con multiples videos que intervienen la imagen de laura richardson"/>
<h2> Introducción </h2>
<p id="introducción">

En Agosto del 2024 pude realizar varias acciones gracias a Sensorialis, un ciclo de conciertos AV que proponen entrelazar la curiosidad científica con la curiosidad artística. En ese marco la edición en la que participé se llamó <b>Diásporas Nativas</b>, y se realizó conjuntamente con el Museo de Ciencias Naturales Bernardino Rivadavia. Como parte de la propuesta conocimos las diversas colecciones botánicas de ese museo, y pudimos charlar directamente con los grupos de investigación que las estudian, conservan y hacen crecer. Luego realizamos un concierto AV en el Centro Cultural de España en Buenos Aires, en CABA, Argentina.

<img src="flyer.png"/>
<br>
<br>
En la visita, además de entrar en contacto con diversos equipos de cientifiques y conocer su labor, buscamos hacer dialogar <i>algo</i> de esas colecciones y esa visita con nuestras líneas de producción/investigación artísticas, para que estuviera presente de alguna forma en el concierto. 
<br>
<br>
De esa experiencia muchas cosas quedaron resonando en mí - entre ellas la colección de paleobotánica con plantas y frutos fosilizados de hace millones de años y de distintos lugares de la Argentina, y la colección botánica de las Islas Malvinas con especímenes recolectados a principios de siglo XX. 
<br>
<br>
<img src="coleccionmalvinas.gif"/>
<img src="./laura_fecha.gif"/>

<br>
<br>
Pensaba en lo lejano que puede ser o estar o sentirse lo nuestro (lo que compartimos y nos hace). 
<br>
<br>
Entre lo cercano y cercano hay relaciones de contigüidad. Tienen lo suyo, pero me parecen más interesantes las interacciones a distancia. Una pispeada de cómo poder captarlas son, por ejemplo, lo que cambió el panorama de la Inteligencia Artificial generativa allá en el 2019 con el mecanismo de Atención.
<br>
<br>
Luego de la visita pensaba en las relaciones entre lo cercano y lo lejano, y el rol de estas colecciones, la ciencia y el arte respecto a eso. Las relaciones entre las partes y el todo, y en hacer obra cerrada/"completa" y otra abiertamente parcial.
<br>
<br>
El livecoding de por sí <b> puede ser vivido como una experiencia de lectura/escritura muy abierta</b>, y es lo que más busco.
<br>
<br>
<i>> El livecoding es la práctica de improvisación de código en vivo, en donde se muestra el texto del código a medida de que se construye. Yo hago livecoding visual sobre todo, por lo que muestro el código que sintetiza imagen y transforma videos.</i>
<br>
<br>
Desde que empecé a livecodear investigo su potencial para <b>exigir</b> procesamiento por parte de las personas que participan. Esto implica darles momentos con más pistas: código simple que si se lee e interpreta como texto de lenguaje natural puede relacionarse directamente con las imágenes que generan.... Todo esto para luego aumentar el ritmo y complejidad de los cambios y de la propuesta. 

<br>
<br>
<i>Así transcurro una performance de livecoding, leyendolé las caras a las personas a ver qué tipo de intereses demuestran. Si hay personas a las que les interesa develar el mecanismo, el juego es más interesante, y es exigente para mí también - porque me exige a buscar el código más simple que haga la transformación visual que quiero, me obliga a poner buenos nombres de funciones y variables, y a mantener un entorno limpio sin código muerto. <b>De esa forma pienso la performance como el armado de circuitos.</b></i>
<img src="karen.gif"/>
<br>
<br>
Parte de esta búsqueda es la construcción continua de mi parte de instrumentos algorítmicos que le agreguen capas a ese proceso.
<br>
<br>
Me interesa tejer discursos pero yendo un poco más allá, tejer nuevos lenguajes para decir e incluso armar el compilador yo. Me parece un honor acercarse a poder sugerir relaciones entre las partes y el todo, lo pequeño y lo grande, lo del día a día y lo que es construido como histórico. Para eso hice <b> Cóndor Loop. </b>

</p>
<h2> Cóndor Loop </h2>
<p>
<img src="mostrar.gif"/>
Existen relaciones que me interesan tejer.
<br>
<br>
Por ej: Laura Richardson hablando sobre litio, petróleo y agua dulce de Argentina. Hay una relación directa entre ese discurso del 2023, y que les argentines solo podamos tocar y conocer las plantas de Malvinas visitando las colecciones botánicas de museos.
<br>
<br>
<img src="./reconstruccion.png" alt="Screenshot"/>
<br>
<br>
Me interesaba poder plantear un sistema-instrumento que ponga a estas relaciones en primer plano, y que además se preste a <b>juegos visuales-conceptuales de ocultamiento/develamiento rítmicos.</b>
<br>
<br>
Para esto activé registros fotográficos que tomé de la visita guiada a las diversas colecciones botánicas que son campo de investigación en el Museo Argentino de Ciencias Naturales Bernardino Rivadavia, y que constituyen un pequeño dataset. Además utilicé el Nodo de Cómputo de San Francisco, Córdoba, para poder hacer todo el procesamiento necesario para el sistema.

Esta es la metodología que armé - algunas partes están automatizadas, otras son más manuales. Utilicé python para todo.
<br>
<br>
<img src="diagrama.png"/>
Lo veamos paso por paso.
<br>
<br>

Pensemos el proceso para una foto, ya que después esto se repite equivalentemente para todas las fotos que saqué en el Museo.
<br>
<br>
Sea una foto,
<br>
<br>
Por ejemplo:
<img src="malvinas_s.jpg"/>
<br>
<br>
Esta foto es fragmentada en 24 recortes aleatorios. En este proceso se guardan las posiciones originales de estos recortes.
<br>
<br>
<img src="recortes.png"/>
<br>
<br>
Cada recorte es pasado por un modelo que toma una imágen y devuelve un video utilizando Inteligencia Artificial. Por atrás este modelo está buscando darle moción a la imagen de una manera probabilística. Por cada imagen hay 24 videos que realizar, correspondientes a los 24 retazos o recortes realizados antes. Para que haya una idea de tiempos de cómputo, un video de 5 segundos y de resolución 512x512 píxeles le toma al Nodo de Cómputo ~2 minutos en realizar. O sea que animar 24 videos (el equivalente en mi sistema a una “imagen”) toma casi 50 minutos (<i>si sale todo bien</i>). <b>En resumen, un "video" completo (o sea, hecho de 24 videos-retazos-hojas) que dura 5 segundos toma una hora en sintetizar</b>. Esta parte del proceso fue increiblemente lenta ya que implicó idas y vueltas hasta encontrar parámetros del proceso que me gusten, e, incluso con acceso a cómputo científico, tuve que automatizar partes de la interacción con el modelo de manera programática - que no fue sencillo ya que este tipo de uso no está documentado online. 
<br>
Ejemplo de un video-recorte pasado por IA - lo que sería una hojita del racimo: 
<img src="recorte.gif"/>

<br>
Cada video-recorte está asociado a la imagen original y tiene guardado las posiciones a las que corresponden.
<br>
<br>
La loopera Cóndor tiene un secuenciador de conjuntos de videos: Toma un directorio de video-recortes y sabe ejecutar los 24 videos en simultaneo, que al posicionarlos en las coordenadas originales vuelven a armar fragmentariamente la imagen original (o una parte, dependiendo de cómo se hizo el recorte)
<br>
<br>
<img src="condor.gif"/>

En Cóndor, un “video” está formado por videos. La gran mayoría de los recortes de por sí tienen poca información. Para entender lo que ha sido representado originalmente hay que ver los 24 videos en simultáneo.
<br>
<br>

Cóndor me permite elegir si mostrar de a un video-retazo por vez, de a un par o todos a la vez, completando el video-racimo. Además me permite mezclar video-retazos que pertenecen a distintos video-racimos. También puedo configurar cada cuánto cambiar al próximo conjunto de videos, para controlar más finamente la sensación rítmica.
<br>
<br>
</p>
<h2> Looperas generativas </h2>
<p>
Pensaba en racimos de videos, porque pensaba en relaciones semánticas entre videos y lo que viene siendo una obsesión con modalidad fuego corona hace años: una <i>loopera semántica</i>. Debo haber implementado al menos 10 looperas completas en diversos lenguajes a lo largo de estos últimos tres años. Hice ya varias en python, varias en javascript, algunas en c++, un par en rust. Todas son software libre y se pueden encontrar en mi <a target="_blank" href="https://www.github.com/karen-pal">github</a>.
<br>
<br>
Sabemos que un video es una secuencia de imágenes. Si un video es un racimo de "imágenes" conectados semánticamente, entonces el concepto que los une es una vaina. Puede ser que entre varios videos haya conexiones semánticas también, como sería un tema respecto a un concepto, en cuyo caso tendríamos un tallo de una planta. Si yo pudiera extraer ese “tema” o “concepto” que los conecta, luego podría operar directamente entre conceptos para formar oraciones, párrafos, textos - en un sentido abstracto - y rearmar la planta entera, generativamente, al conectar las transformaciones conceptuales con la variable tiempo. Complejizar incluso la variable tiempo y pensar en los tiempos de las plantas, o los tiempos circulares de la naturaleza. Incluso, si pudiera medir la fortaleza de las relaciones semánticas - o los "grados de separación" podría pensar en modelar distintas morfologías - qué sería un árbol versus un arbusto, una flor versus una inflorescencia? 
<img src="morfologias.jpg"/>
<br>
<br>
Por ahora todo esto es un sueño. Mi loopera soñada.
<br>
<br>
En una performance con una loopera -la que tenga- lo estoy haciendo, pero el procesamiento lo hago yo en mi hardware biológico. Todavía no automaticé esta parte ya que es un desafío técnico bastante duro.
<br>
<br>
Me gusta porque es relacionar video<->texto<->imagen y subir un nivel de abstracción que permite narración en una performance visual, pero además permite el armado de metalenguajes y meta(*)lenguajes (en un runway process).
<br>
Este tema también lo exploré en <i>árbol</i> - un instrumento basado en Procesamiento de Lenguaje Natural para hacer performance visual junto a la banda Marmotas Dreams, y que fue estrenado en el Centro de Arte Sonoro (CASo).

<div class="video-responsive">
	<iframe width="560" height="315" src="https://www.youtube.com/embed/edd1f2PWwZk?si=0ci2IirOHx_qBY1I" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>
<img src="language.png"/>
</p>
<p>
<br>
<br>
Como yo pienso lo que hago cuando performo visuales como <b>montaje en vivo</b>, siempre busco tener formas de operar semánticamente - aunque las relaciones semánticas hayan sido codificadas con reglas todavía, o hayan sido pre-calculadas - en este caso <b>armé previamente secuencias de video-racimos que, en mi opinión, al sucederse uno atrás del otro construyen una imagen mental más amplia que aquella que une puede tener si viera solamente un fragmento.</b>
<br>
<br>

Esta acción se corresponde con mi meta-proyecto <i>¿Qué sueña la Argentina?</i>. Dentro de este proyecto está Lenguaje Frontera, que ganó el apoyo a la producción del cceba el año pasado y será exhibido este en el recoleta. Además está la performance que hice a principios de este año en Underclub llamado Plan Condor AV. Está también Trabajo, que ganó el primer premio del Premio Itaú categoría Arte e IA.
<br>
<br>


<img src="./laura.png" alt="Screenshot de la pantalla mostrando un sistema con multiples videos que intervienen la imagen de laura richardson"/>


</p>

<h1> Acá, en el enésimo Plan Cóndor. </h1>
<p>
<img src="grupo.jpg"/>
</p>






  </body>
  <footer id="final"> <h1>
      > Agosto 2024. 
<br>
      <a href="https://www.instagram.com/kardaver/" >Karen Palacio aka kardaver. </a>
<br>
      <a href="https://www.youtube.com/@karenpalacio" >Canal de Youtube</a>
<br>
      <a href="https://karen-pal.github.io/about/" >Página personal</a>
<br>
      </h1>
  </footer>
  <script>console.log("Buscabas algo?")</script>
  </body>
</html>
